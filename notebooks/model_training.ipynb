{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-like script: Model training, tuning, MLflow logging, evaluation (robust to Task 3/4 availability)\n",
    "# Usage: run this in a Jupyter cell or as a script. Adjust src path if necessary.\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "# ensure local src is importable\n",
    "src_path = str((Path('..') / 'src').resolve())\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Project imports (model training utilities)\n",
    "from model_training import (\n",
    "    prepare_data, build_default_models, default_param_grids,\n",
    "    make_pipeline_with_scaler, fit_and_tune, evaluate_model,\n",
    "    log_experiment_mlflow, save_model_local\n",
    ")\n",
    "from proxy_target import ProxyTargetEngineer  # Task 4 helper (used if target missing)\n",
    "\n",
    "# Configuration\n",
    "RAW_TX_PATH = Path('..') / 'data' / 'raw' / 'data.csv'\n",
    "PROCESSED_DIR = Path('..') / 'data' / 'processed'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FEATURES_WITH_PROXY = PROCESSED_DIR / 'features_with_proxy.csv'\n",
    "FEATURES_BASE = PROCESSED_DIR / 'features.csv'\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "CANDIDATE_MODELS = [\"logistic\", \"random_forest\"]  # run at least two\n",
    "\n",
    "# Helper: create proxy target and merge into feat_df\n",
    "def ensure_features_with_proxy(feat_df: pd.DataFrame, raw_tx_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure feat_df contains 'is_high_risk'. If missing, compute using ProxyTargetEngineer\n",
    "    and raw transaction-level data at raw_tx_path. Returns updated feat_df.\n",
    "    \"\"\"\n",
    "    if \"is_high_risk\" in feat_df.columns:\n",
    "        print(\"is_high_risk already present in features. No action needed.\")\n",
    "        return feat_df\n",
    "\n",
    "    if not raw_tx_path.exists():\n",
    "        raise FileNotFoundError(f\"Raw transaction data required to create proxy target not found: {raw_tx_path}\")\n",
    "\n",
    "    print(\"is_high_risk missing â€” computing proxy target using RFM + KMeans...\")\n",
    "    raw_tx = pd.read_csv(raw_tx_path)\n",
    "    proxy = ProxyTargetEngineer(\n",
    "        id_col=\"CustomerId\",\n",
    "        time_col=\"TransactionStartTime\",\n",
    "        amount_col=\"Amount\",\n",
    "        n_clusters=3,\n",
    "        random_state=RANDOM_STATE,\n",
    "        scale=True\n",
    "    )\n",
    "    proxy.fit(raw_tx)\n",
    "    feat_df_with_target = proxy.assign_labels(feat_df)\n",
    "    print(\"Assigned proxy target; distribution:\")\n",
    "    print(feat_df_with_target[\"is_high_risk\"].value_counts(dropna=False).to_string())\n",
    "    return feat_df_with_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Load or build features ----------\n",
    "# 1) Prefer a ready-made features_with_proxy.csv\n",
    "if FEATURES_WITH_PROXY.exists():\n",
    "    print(f\"Loading processed features with proxy target from: {FEATURES_WITH_PROXY}\")\n",
    "    df = pd.read_csv(FEATURES_WITH_PROXY)\n",
    "else:\n",
    "    # 2) Try to load base features.csv (Task 3 output) and then compute proxy target (Task 4)\n",
    "    if FEATURES_BASE.exists():\n",
    "        print(f\"Loading base features from: {FEATURES_BASE}\")\n",
    "        df = pd.read_csv(FEATURES_BASE)\n",
    "        # ensure target present or compute\n",
    "        if 'is_high_risk' not in df.columns:\n",
    "            df = ensure_features_with_proxy(df, RAW_TX_PATH)\n",
    "            # persist\n",
    "            df.to_csv(FEATURES_WITH_PROXY, index=False)\n",
    "            print(f\"Saved features_with_proxy.csv to: {FEATURES_WITH_PROXY}\")\n",
    "    else:\n",
    "        # 3) As a last resort, try to build features via a pipeline module if available\n",
    "        try:\n",
    "            from feature_engineering_4_PTV import feature_engineering_pipeline\n",
    "            print(\"Building features using feature_engineering_4_PTV.feature_engineering_pipeline...\")\n",
    "            raw_tx = pd.read_csv(RAW_TX_PATH)\n",
    "            df, feat_desc = feature_engineering_pipeline(raw_tx, categorical_cols=None, create_proxy_target=False)\n",
    "            # save base features\n",
    "            df.to_csv(FEATURES_BASE, index=False)\n",
    "            print(f\"Saved base features to {FEATURES_BASE}\")\n",
    "            # compute proxy target\n",
    "            df = ensure_features_with_proxy(df, RAW_TX_PATH)\n",
    "            df.to_csv(FEATURES_WITH_PROXY, index=False)\n",
    "            print(f\"Saved features_with_proxy.csv to: {FEATURES_WITH_PROXY}\")\n",
    "        except Exception as exc:\n",
    "            raise FileNotFoundError(\n",
    "                \"No processed features found, pipeline unavailable, and cannot continue. \"\n",
    "                \"Provide ../data/processed/features_with_proxy.csv or ../data/processed/features.csv, \"\n",
    "                \"or ensure feature_engineering_4_PTV is importable.\"\n",
    "            ) from exc\n",
    "\n",
    "print(\"Final features shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"Target distribution (if present):\")\n",
    "if \"is_high_risk\" in df.columns:\n",
    "    print(df[\"is_high_risk\"].value_counts(dropna=False).to_string())\n",
    "else:\n",
    "    raise RuntimeError(\"is_high_risk target not present after attempted creation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Data preparation ----------\n",
    "X_train, X_test, y_train, y_test = prepare_data(df, target_col=\"is_high_risk\", test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n",
    "print(\"Train target distribution:\")\n",
    "print(y_train.value_counts(normalize=True).to_string())\n",
    "\n",
    "# ---------- Model selection, tuning, training ----------\n",
    "models = build_default_models(random_state=RANDOM_STATE)\n",
    "param_grids = default_param_grids()\n",
    "\n",
    "# Directory to save models\n",
    "MODELS_DIR = Path('..') / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "for key in CANDIDATE_MODELS:\n",
    "    clf = models.get(key)\n",
    "    if clf is None:\n",
    "        print(f\"Model '{key}' not found in available models. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Training & tuning model: {key} ===\")\n",
    "    pipe = make_pipeline_with_scaler(clf)\n",
    "    params = param_grids.get(key, {})\n",
    "\n",
    "    # Tune with GridSearch (fallback to fit if tuning fails)\n",
    "    try:\n",
    "        search = fit_and_tune(pipe, params, X_train, y_train, cv=3, search_type=\"grid\", scoring=\"roc_auc\")\n",
    "        best = search.best_estimator_\n",
    "        best_params = search.best_params_\n",
    "    except Exception as e:\n",
    "        print(f\"Hyperparameter tuning failed for {key} (error: {e}). Falling back to default fit.\")\n",
    "        best = pipe.fit(X_train, y_train)\n",
    "        best_params = getattr(best, \"get_params\", lambda: {})()\n",
    "\n",
    "    metrics = evaluate_model(best, X_test, y_test)\n",
    "    print(f\"Evaluation metrics for {key}:\")\n",
    "    pprint.pprint(metrics)\n",
    "    # Log experiment to MLflow\n",
    "    # Save model locally\n",
    "    model_path = MODELS_DIR / f\"{key}_best.pkl\"\n",
    "    save_model_local(best, str(model_path))\n",
    "\n",
    "    # Log to MLflow (if configured)\n",
    "    run_id = log_experiment_mlflow(\n",
    "        name=\"PTV_modeling\",\n",
    "        estimator=best,\n",
    "        params=best_params,\n",
    "        metrics=metrics,\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        artifacts={\"model_pkl\": str(model_path)},\n",
    "        model_save_path=str(model_path),\n",
    "        register_name=f\"PTV-{key}\"\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"model\": key,\n",
    "        \"best_params\": best_params,\n",
    "        \"metrics\": metrics,\n",
    "        \"run_id\": run_id,\n",
    "        \"model_path\": str(model_path)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Compare and report ----------\n",
    "print(\"\\n=== All results ===\")\n",
    "pprint.pprint(results)\n",
    "\n",
    "# choose best by roc_auc (fallback to f1)\n",
    "def score_for(r):\n",
    "    m = r[\"metrics\"]\n",
    "    return m.get(\"roc_auc\", m.get(\"f1\", float(\"-inf\")))\n",
    "\n",
    "if results:\n",
    "    best_result = max(results, key=score_for)\n",
    "    print(\"\\nBest model overall (by ROC-AUC or fallback):\")\n",
    "    pprint.pprint(best_result)\n",
    "    print(f\"Best model saved at: {best_result['model_path']}\")\n",
    "else:\n",
    "    print(\"No model results to compare.\")\n",
    "\n",
    "print(\"\\nModel training script complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbe566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
