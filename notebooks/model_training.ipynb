{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-like script: Model training, tuning, MLflow logging, evaluation\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ensure src on path\n",
    "src_path = str((Path('..') / 'src').resolve())\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from data_processing import DataLoader\n",
    "from model_training import (\n",
    "    prepare_data, build_default_models, default_param_grids,\n",
    "    make_pipeline_with_scaler, fit_and_tune, evaluate_model,\n",
    "    log_experiment_mlflow, save_model_local\n",
    ")\n",
    "from proxy_target import ProxyTargetEngineer\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# 1. Load processed features (with is_high_risk)\n",
    "data_path = Path('..') / 'data' / 'processed' / 'features_with_proxy.csv'\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Processed features not found: {data_path}\")\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Loaded processed features:\", df.shape)\n",
    "\n",
    "# 2. Prepare data\n",
    "X_train, X_test, y_train, y_test = prepare_data(df, target_col=\"is_high_risk\", test_size=0.2, random_state=42)\n",
    "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "# 3. Build models and param grids\n",
    "models = build_default_models(random_state=42)\n",
    "param_grids = default_param_grids()\n",
    "\n",
    "# choose two model keys to run (at least two)\n",
    "candidates = [\"logistic\", \"random_forest\"]\n",
    "\n",
    "results = []\n",
    "for key in candidates:\n",
    "    clf = models[key]\n",
    "    # wrap in pipeline (scaler+clf) for logistic; RF can be left unscaled but pipeline is fine\n",
    "    pipe = make_pipeline_with_scaler(clf)\n",
    "    params = param_grids.get(key, {})\n",
    "    print(f\"Training & tuning {key} ...\")\n",
    "    search = fit_and_tune(pipe, params, X_train, y_train, cv=3, search_type=\"grid\", scoring=\"roc_auc\")\n",
    "    best = search.best_estimator_\n",
    "    metrics = evaluate_model(best, X_test, y_test)\n",
    "    # save model locally\n",
    "    model_path = f\"../models/{key}_best.pkl\"\n",
    "    save_model_local(best, model_path)\n",
    "    # log to mlflow (if available)\n",
    "    run_id = log_experiment_mlflow(\n",
    "        name=\"PTV_modeling\",\n",
    "        estimator=best,\n",
    "        params=search.best_params_,\n",
    "        metrics=metrics,\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        artifacts={\"model_pkl\": model_path},\n",
    "        model_save_path=model_path,\n",
    "        register_name=f\"PTV-{key}\"\n",
    "    )\n",
    "    results.append({\"model\": key, \"best_params\": search.best_params_, \"metrics\": metrics, \"run_id\": run_id})\n",
    "    print(f\"Done {key}, metrics:\")\n",
    "    pprint.pprint(metrics)\n",
    "\n",
    "# 4. Compare results\n",
    "print(\"All results:\")\n",
    "pprint.pprint(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
