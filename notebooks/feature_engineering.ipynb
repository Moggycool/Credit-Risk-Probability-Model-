{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc53e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feature Engineering Notebook as Python Script\n",
    "Run this script to execute the entire feature engineering pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# Feature Engineering: Customer-Level Feature Creation\n",
    "# This notebook performs feature engineering on the raw transaction data.\n",
    "# It loads the raw data, applies feature engineering transformations,\n",
    "# and visualizes key aspects of the engineered features.\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# Feature Engineering Rationale\n",
    "# \n",
    "# ### 1. Aggregate Features\n",
    "# - **Overall aggregates**: Capture long-term customer behavior patterns\n",
    "# - **Recent 30-day aggregates**: Identify recent changes in spending patterns\n",
    "# - **Statistical moments**: Mean, std, skew to understand distribution characteristics\n",
    "# \n",
    "# ### 2. Temporal Features\n",
    "# - **Cyclical encoding**: sin/cos transformations for hour/month to handle cyclical nature\n",
    "# - **Velocity metrics**: Transactions per day over 1/7/30 day windows to detect behavioral changes\n",
    "# - **Recency metrics**: Time since last transaction for inactivity detection\n",
    "# - **Time-based indicators**: Weekend vs weekday, business hours vs off-hours\n",
    "# \n",
    "# ### 3. Encoding Strategies\n",
    "# - **One-hot encoding**: For low-cardinality features (<8 categories) to avoid curse of dimensionality\n",
    "# - **Frequency encoding**: For high-cardinality features to preserve information while reducing dimensionality\n",
    "# - **WoE encoding**: For categorical features with respect to fraud target, provides interpretable monotonic relationships\n",
    "# \n",
    "# ### 4. Normalization & Transformation\n",
    "# - **log1p transformation**: Handles right-skewed distributions common in financial data\n",
    "# - **StandardScaler**: Standardizes features for linear models\n",
    "# - **Correlation filtering**: Removes highly correlated features (>0.95) to reduce multicollinearity\n",
    "# \n",
    "# ### 5. WoE & Information Value (IV)\n",
    "# - **Monotonic binning**: Ensures monotonic relationship between binned features and target\n",
    "# - **IV thresholds**: \n",
    "#   - IV < 0.02: No predictive power (discarded)\n",
    "#   - 0.02 ≤ IV < 0.1: Weak predictive power\n",
    "#   - 0.1 ≤ IV < 0.3: Medium predictive power\n",
    "#   - IV ≥ 0.3: Strong predictive power\n",
    "# - **Regulatory compliance**: WoE provides interpretable feature importance for model validation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INITIALIZING FEATURE ENGINEERING PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Feature Engineering: Customer-Level Feature Creation\n",
    "# This notebook performs feature engineering on the raw transaction data.\n",
    "# It loads the raw data, applies feature engineering transformations,\n",
    "# and visualizes key aspects of the engineered features.\n",
    "\n",
    "# Ensure src is importable (relative to script location)\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "try:\n",
    "    from src.data_processing import DataLoader\n",
    "    from src.feature_engineering import feature_engineering_pipeline, WoEIVCalculator\n",
    "    print(\"✓ Successfully imported required modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Error importing modules: {e}\")\n",
    "    print(\"Please ensure the src directory contains:\")\n",
    "    print(\"  - data_processing.py with DataLoader class\")\n",
    "    print(\"  - feature_engineering.py with feature_engineering_pipeline function\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd259505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 1. Load raw transaction data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 1: LOADING RAW TRANSACTION DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "data_path = PROJECT_ROOT.parent / 'data' / 'raw' / 'data.csv'\n",
    "if not data_path.exists():\n",
    "    # Try alternative locations\n",
    "    alternative_paths = [\n",
    "        PROJECT_ROOT / 'data' / 'raw' / 'data.csv',\n",
    "        PROJECT_ROOT.parent / 'data.csv',\n",
    "        PROJECT_ROOT / 'data.csv',\n",
    "    ]\n",
    "    \n",
    "    for alt_path in alternative_paths:\n",
    "        if alt_path.exists():\n",
    "            data_path = alt_path\n",
    "            break\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"⚠️ Warning: Raw data file not found: {data_path}\")\n",
    "        print(\"Creating sample data for demonstration...\")\n",
    "        # Create sample data\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        data = {\n",
    "            'CustomerId': [f'CUST_{i%100:03d}' for i in range(n_samples)],\n",
    "            'Amount': np.random.lognormal(4, 1.5, n_samples),\n",
    "            'Value': np.random.lognormal(3, 1.2, n_samples),\n",
    "            'TransactionStartTime': pd.date_range('2024-01-01', periods=n_samples, freq='H'),\n",
    "            'ProductCategory': np.random.choice(['A', 'B', 'C', 'D'], n_samples),\n",
    "            'ChannelId': np.random.choice(['Web', 'Mobile', 'In-Store'], n_samples),\n",
    "            'ProviderId': np.random.choice(['P1', 'P2', 'P3', 'P4', 'P5'], n_samples),\n",
    "            'FraudResult': np.random.choice([0, 1], n_samples, p=[0.95, 0.05])\n",
    "        }\n",
    "        raw_df = pd.DataFrame(data)\n",
    "        print(\"✓ Created sample data with 1000 transactions\")\n",
    "    else:\n",
    "        loader = DataLoader(str(data_path))\n",
    "        raw_df = loader.load_data()\n",
    "        print(f\"✓ Loaded data from alternative path: {data_path}\")\n",
    "else:\n",
    "    loader = DataLoader(str(data_path))\n",
    "    raw_df = loader.load_data()\n",
    "    print(f\"✓ Loaded data from: {data_path.resolve()}\")\n",
    "\n",
    "print(f\"Raw data shape: {raw_df.shape}\")\n",
    "print(f\"Number of unique customers: {raw_df['CustomerId'].nunique()}\")\n",
    "print(f\"Date range: {raw_df['TransactionStartTime'].min()} to {raw_df['TransactionStartTime'].max()}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of raw data:\")\n",
    "print(raw_df.head())\n",
    "\n",
    "# Check for missing values\n",
    "missing = raw_df.isna().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "if not missing.empty:\n",
    "    print(\"\\nColumns with missing values (count):\")\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"\\n✓ No missing values found in raw data.\")\n",
    "\n",
    "print('\\nColumns and dtypes:')\n",
    "print(raw_df.dtypes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81bc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 2. Run feature engineering pipeline\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: RUNNING FEATURE ENGINEERING PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "## Aggregate, Temporal, Encoded Feature Creation & WoE/IV integration\n",
    "\n",
    "# Only keep categorical columns that exist in raw_df\n",
    "auto_cats = [col for col in ['ProductCategory', 'ChannelId', 'ProviderId'] if col in raw_df.columns]\n",
    "print(f\"Categorical columns used: {auto_cats}\")\n",
    "\n",
    "feat_df, feat_desc = feature_engineering_pipeline(\n",
    "    raw_df, categorical_cols=auto_cats\n",
    ")\n",
    "\n",
    "print(f\"✓ Engineered features shape: {feat_df.shape}\")\n",
    "print(f\"\\nFirst 10 rows of engineered features:\")\n",
    "print(feat_df.head(10))\n",
    "\n",
    "# Save for downstream steps\n",
    "output_dir = PROJECT_ROOT.parent / 'data' / 'processed'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "feat_df.to_csv(output_dir / \"task-3-features.csv\", index=False)\n",
    "feat_desc.to_csv(output_dir / \"task-3-features-description.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Features saved to: {output_dir / 'task-3-features.csv'}\")\n",
    "print(f\"✓ Feature descriptions saved to: {output_dir / 'task-3-features-description.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5919b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Feature Analysis\n",
    "# \n",
    "# ### Feature Categories Created:\n",
    "# 1. **Aggregate Features**: Sum, mean, max, std, skew of Amount and Value\n",
    "# 2. **Recent Activity**: 30-day window aggregates\n",
    "# 3. **Temporal Features**: Hour, weekday, month, year statistics\n",
    "# 4. **Cyclical Features**: sin/cos transformations for hour and month\n",
    "# 5. **Velocity Features**: Transaction rates over 1/7/30 days\n",
    "# 6. **Recency Features**: Time since last transaction\n",
    "# 7. **Categorical Encodings**: One-hot and frequency encodings\n",
    "# 8. **WoE Features**: Weight of Evidence encodings (if FraudResult available)\n",
    "# 9. **Normalized Features**: log-transformed and standardized numeric features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: FEATURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "## Feature Description Table\n",
    "print(\"Feature Description Table (first 20 rows):\")\n",
    "print(feat_desc.head(20))\n",
    "\n",
    "print(f\"\\nTotal features created: {len(feat_desc)}\")\n",
    "print(f\"Features with IV values: {feat_desc['IV_Value'].notna().sum()}\")\n",
    "\n",
    "## WoE/IV Feature Analysis\n",
    "woe_cols = [col for col in feat_df.columns if col.endswith(\"_woe\")]\n",
    "if woe_cols:\n",
    "    print(f\"\\n✓ WoE Features created ({len(woe_cols)}):\", woe_cols)\n",
    "    \n",
    "    # Show IV values for WoE features\n",
    "    iv_rows = feat_desc[feat_desc['Feature'].str.endswith(\"_woe\")]\n",
    "    if not iv_rows.empty:\n",
    "        print(\"\\nInformation Values (IV) for WoE features:\")\n",
    "        print(iv_rows[['Feature', 'IV_Value']])\n",
    "        \n",
    "        print(\"\\nIV Interpretation Guide:\")\n",
    "        print(\"  IV < 0.02: Not useful for prediction\")\n",
    "        print(\"  0.02 ≤ IV < 0.1: Weak predictor\")\n",
    "        print(\"  0.1 ≤ IV < 0.3: Medium predictor\")\n",
    "        print(\"  IV ≥ 0.3: Strong predictor\")\n",
    "    \n",
    "    # Plot distribution of WoE features\n",
    "    n_woe = len(woe_cols)\n",
    "    if n_woe > 0:\n",
    "        fig, axes = plt.subplots((n_woe + 1) // 2, 2, figsize=(12, 4*((n_woe + 1)//2)))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for ax, col in zip(axes, woe_cols):\n",
    "            sns.histplot(feat_df[col].dropna(), bins=20, ax=ax, kde=True)\n",
    "            ax.set_title(f'WoE Feature: {col}')\n",
    "            ax.set_xlabel('Weight of Evidence')\n",
    "            \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(woe_cols), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "            \n",
    "        plt.suptitle('Weight of Evidence (WoE) Feature Distributions', fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nℹ️ No WoE features found (FraudResult column may not be present in raw data).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Numeric Feature Distributions: Before and After Transformations\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: NUMERIC FEATURE DISTRIBUTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# BEFORE transformation: All raw numeric features\n",
    "num_cols_before = [\n",
    "    col for col in raw_df.select_dtypes(include=[float, int]).columns\n",
    "    if not col.lower().startswith(('id', 'date', 'time')) and col not in ['FraudResult']\n",
    "]\n",
    "\n",
    "if not num_cols_before:\n",
    "    print('ℹ️ No numeric feature columns found in raw data.')\n",
    "else:\n",
    "    print(f\"Raw numeric features ({len(num_cols_before)}): {num_cols_before}\")\n",
    "    \n",
    "    fig_width = min(16, 5 * len(num_cols_before))\n",
    "    fig_height = 5\n",
    "    fig_before, axes_before = plt.subplots(1, len(num_cols_before), figsize=(fig_width, fig_height))\n",
    "    if len(num_cols_before) == 1:\n",
    "        axes_before = [axes_before]\n",
    "        \n",
    "    for i, col in enumerate(num_cols_before):\n",
    "        sns.histplot(raw_df[col].dropna(), bins=40, kde=True, ax=axes_before[i], color='tab:blue')\n",
    "        axes_before[i].set_title(f\"{col}\\n(Raw)\", fontsize=12)\n",
    "        axes_before[i].set_xlabel('Value', fontsize=10)\n",
    "        axes_before[i].set_ylabel('Count', fontsize=10)\n",
    "        \n",
    "    fig_before.suptitle(\"Numeric Feature Distributions (Raw Data)\", fontsize=14, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# AFTER transformation: All engineered numeric features (log/std)\n",
    "num_cols_after = [\n",
    "    col for col in feat_df.select_dtypes(include=[float, int]).columns\n",
    "    if not col.lower().startswith('customerid') and\n",
    "       ('_log_std' in col or '_log' in col or '_std' in col)\n",
    "]\n",
    "if not num_cols_after:\n",
    "    print('ℹ️ No transformed numeric columns found in engineered features.')\n",
    "else:\n",
    "    if len(num_cols_after) > 5:\n",
    "        print(f\"\\nTransformed numeric features ({len(num_cols_after)}): {num_cols_after[:5]}...\")\n",
    "    else:\n",
    "        print(f\"\\nTransformed numeric features: {num_cols_after}\")\n",
    "    \n",
    "    fig_width = min(16, 5 * min(3, len(num_cols_after)))  # Show max 3\n",
    "    fig_height = 5\n",
    "    n_plots = min(3, len(num_cols_after))\n",
    "    \n",
    "    fig_after, axes_after = plt.subplots(1, n_plots, figsize=(fig_width, fig_height))\n",
    "    if n_plots == 1:\n",
    "        axes_after = [axes_after]\n",
    "        \n",
    "    for i, col in enumerate(num_cols_after[:n_plots]):\n",
    "        sns.histplot(feat_df[col].dropna(), bins=40, kde=True, ax=axes_after[i], color='tab:green')\n",
    "        base_name = col.replace('_log_std', '').replace('_log', '').replace('_std', '')\n",
    "        axes_after[i].set_title(f\"{base_name}\\n(Transformed)\", fontsize=12)\n",
    "        axes_after[i].set_xlabel('Standardized Value', fontsize=10)\n",
    "        axes_after[i].set_ylabel('Count', fontsize=10)\n",
    "        \n",
    "    fig_after.suptitle(\"Numeric Feature Distributions (After log1p + StandardScaler)\", fontsize=14, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTransformation applied: log1p + StandardScaler\")\n",
    "    print(\"Purpose: Handle right-skewed distributions and standardize for linear models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98494c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Calendar (Day/Month/Year) temporal feature check\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 5: TEMPORAL FEATURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "temporal_cols = [c for c in feat_df.columns if any(k in c for k in ['Day_', 'Month_', 'Year_', 'Hour_', 'Weekday_'])]\n",
    "print(f\"Extracted temporal features ({len(temporal_cols)}):\")\n",
    "for col in temporal_cols[:10]:  # Show first 10\n",
    "    print(f\"  - {col}\")\n",
    "if len(temporal_cols) > 10:\n",
    "    print(f\"  ... and {len(temporal_cols) - 10} more\")\n",
    "    \n",
    "# Show sample of temporal features\n",
    "sample_temporal = [c for c in temporal_cols if any(x in c for x in ['mean', 'mode', 'std'])]\n",
    "print(f\"\\nSample temporal features with CustomerId:\")\n",
    "print(feat_df[sample_temporal[:5] + ['CustomerId']].head())\n",
    "\n",
    "# Plot distributions of key temporal features\n",
    "key_temporal = [c for c in temporal_cols if 'mean' in c and any(x in c for x in ['Hour', 'Weekday', 'Day', 'Month'])]\n",
    "if key_temporal:\n",
    "    n_plots = min(4, len(key_temporal))\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(4*n_plots, 4))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for i, col in enumerate(key_temporal[:n_plots]):\n",
    "        sns.histplot(feat_df[col].dropna(), bins=20, ax=axes[i])\n",
    "        feature_name = col.split('_')[0]\n",
    "        axes[i].set_title(f\"{feature_name} (Mean)\")\n",
    "        axes[i].set_xlabel(f\"Average {feature_name.lower()}\")\n",
    "        \n",
    "    plt.suptitle(\"Temporal Feature Distributions (Mean Values)\", fontsize=14, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1086e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Boxplots to Visualize Outliers in Raw Numeric Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 6: OUTLIER ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if num_cols_before:\n",
    "    print(\"Outlier Analysis in Raw Numeric Features:\")\n",
    "    print(\"Boxplots show distribution spread and potential outliers (points beyond whiskers).\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(num_cols_before), figsize=(4*len(num_cols_before), 5))\n",
    "    if len(num_cols_before) == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for i, col in enumerate(num_cols_before):\n",
    "        sns.boxplot(y=raw_df[col], ax=axes[i], color='tab:orange')\n",
    "        axes[i].set_title(f\"{col}\\nBoxplot (Raw)\", fontsize=12)\n",
    "        axes[i].set_ylabel('Value')\n",
    "        \n",
    "    plt.suptitle(\"Outlier Detection in Raw Numeric Features\", fontsize=14, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nNote: log1p transformation applied during feature engineering helps mitigate outlier effects.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2184c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Velocity Features Analysis\n",
    "# \n",
    "# Velocity features capture transaction behavior patterns over different time windows:\n",
    "# - **Velocity_1d**: Daily transaction rate (short-term behavior changes)\n",
    "# - **Velocity_7d**: Weekly transaction rate (medium-term patterns)\n",
    "# - **Velocity_30d**: Monthly transaction rate (long-term behavior baseline)\n",
    "# \n",
    "# These features are particularly useful for detecting:\n",
    "# - Sudden spikes in transaction frequency (potential fraud)\n",
    "# - Behavioral changes over time\n",
    "# - Seasonality patterns\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 7: VELOCITY FEATURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze velocity features\n",
    "velocity_cols = [c for c in feat_df.columns if 'Velocity_' in c]\n",
    "if velocity_cols:\n",
    "    print(f\"Velocity features created: {velocity_cols}\")\n",
    "    \n",
    "    # Plot velocity features\n",
    "    fig, axes = plt.subplots(1, len(velocity_cols), figsize=(4*len(velocity_cols), 4))\n",
    "    if len(velocity_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(velocity_cols):\n",
    "        window = col.split('_')[1]\n",
    "        sns.histplot(feat_df[col].dropna(), bins=30, ax=axes[i], kde=True)\n",
    "        axes[i].set_title(f\"Transaction Velocity\\n({window} day window)\")\n",
    "        axes[i].set_xlabel(f\"Avg transactions per {window} days\")\n",
    "        axes[i].set_ylabel('Customer count')\n",
    "    \n",
    "    plt.suptitle('Transaction Velocity Feature Distributions', fontsize=14, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show correlation between velocity features\n",
    "    velocity_corr = feat_df[velocity_cols].corr()\n",
    "    print(\"\\nCorrelation between velocity features:\")\n",
    "    print(velocity_corr)\n",
    "else:\n",
    "    print(\"No velocity features found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd32a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Correlation Heatmap of Engineered Numeric Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 8: CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "numeric_cols = feat_df.select_dtypes(include='number').columns.difference(['CustomerId'])\n",
    "if len(numeric_cols) > 1:\n",
    "    print(f\"Creating correlation heatmap for {len(numeric_cols)} numeric features...\")\n",
    "    print(\"Heatmap helps identify:\")\n",
    "    print(\"  • Highly correlated features (redundant information)\")\n",
    "    print(\"  • Independent features (unique information)\")\n",
    "    print(\"  • Anti-correlated features (inverse relationships)\")\n",
    "    \n",
    "    # Select a subset for readability if too many features\n",
    "    if len(numeric_cols) > 20:\n",
    "        print(f\"\\nToo many features ({len(numeric_cols)}) for clear heatmap.\")\n",
    "        print(\"Selecting top 20 most variable features...\")\n",
    "        variances = feat_df[numeric_cols].var().sort_values(ascending=False)\n",
    "        top_features = variances.head(20).index.tolist()\n",
    "        numeric_cols_subset = top_features\n",
    "    else:\n",
    "        numeric_cols_subset = numeric_cols\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr = feat_df[numeric_cols_subset].corr()\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(corr, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title(\"Correlation Heatmap of Engineered Features\", fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Count highly correlated feature pairs\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr.columns)):\n",
    "        for j in range(i+1, len(corr.columns)):\n",
    "            if abs(corr.iloc[i, j]) > 0.8:\n",
    "                high_corr_pairs.append((corr.columns[i], corr.columns[j], corr.iloc[i, j]))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\nFound {len(high_corr_pairs)} feature pairs with |correlation| > 0.8:\")\n",
    "        for feat1, feat2, corr_val in high_corr_pairs[:5]:  # Show top 5\n",
    "            print(f\"  • {feat1} ↔ {feat2}: {corr_val:.3f}\")\n",
    "        if len(high_corr_pairs) > 5:\n",
    "            print(f\"  ... and {len(high_corr_pairs) - 5} more pairs\")\n",
    "    else:\n",
    "        print(\"\\n✓ No highly correlated feature pairs (|corr| > 0.8) found.\")\n",
    "else:\n",
    "    print(\"Not enough numeric columns for correlation heatmap.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f245fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Barplots: Encoded Categorical Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 9: CATEGORICAL FEATURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cat_encode_cols = [\n",
    "    c for c in feat_df.columns\n",
    "    if any(k in c for k in ['ProductCategory', 'ChannelId', 'ProviderId'])\n",
    "    and c != 'CustomerId' and '_woe' not in c\n",
    "]\n",
    "\n",
    "if cat_encode_cols:\n",
    "    print(f\"Encoded categorical features ({len(cat_encode_cols)}):\")\n",
    "    for col in cat_encode_cols:\n",
    "        encoding_type = \"One-hot\" if any(x in col for x in ['ProductCategory', 'ChannelId', 'ProviderId']) and '_freq' not in col else \"Frequency\"\n",
    "        print(f\"  • {col} ({encoding_type} encoding)\")\n",
    "    \n",
    "    # Plot distributions\n",
    "    n_cat = len(cat_encode_cols)\n",
    "    fig, axes = plt.subplots((n_cat + 2) // 3, 3, figsize=(15, 4*((n_cat + 2)//3)))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, col in zip(axes, cat_encode_cols):\n",
    "        if '_freq' in col:\n",
    "            # Frequency encoded - continuous distribution\n",
    "            sns.histplot(feat_df[col].dropna(), bins=20, ax=ax)\n",
    "            ax.set_title(f'{col}\\n(Frequency Encoding)', fontsize=10)\n",
    "        else:\n",
    "            # One-hot encoded - binary distribution\n",
    "            value_counts = feat_df[col].value_counts().head(10)  # Top 10 values\n",
    "            if len(value_counts) > 0:\n",
    "                value_counts.plot(kind='bar', ax=ax)\n",
    "                ax.set_title(f'{col}\\n(One-hot Encoding)', fontsize=10)\n",
    "                ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        ax.set_xlabel('')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(cat_encode_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Encoded Categorical Feature Distributions', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nEncoding Strategy Summary:\")\n",
    "    print(\"  • One-hot encoding: Used for low-cardinality categoricals (<8 unique values)\")\n",
    "    print(\"  • Frequency encoding: Used for high-cardinality categoricals (≥8 unique values)\")\n",
    "    print(\"  • WoE encoding: Applied to categoricals when FraudResult target is available\")\n",
    "else:\n",
    "    print(\"No encoded categorical features found to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc102a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Summary and Next Steps\n",
    "# \n",
    "# ### Feature Engineering Completed:\n",
    "# ✅ **Aggregate Features**: Customer-level statistics for Amount and Value  \n",
    "# ✅ **Temporal Features**: Time-based patterns including velocity and recency  \n",
    "# ✅ **Encoding Strategies**: One-hot, frequency, and WoE encoding  \n",
    "# ✅ **Normalization**: log1p + StandardScaler transformations  \n",
    "# ✅ **WoE/IV Calculation**: Feature importance with regulatory interpretability  \n",
    "# ✅ **Documentation**: Complete feature descriptions with transformations  \n",
    "# \n",
    "# ### Output Artifacts:\n",
    "# 1. `task-3-features.csv`: Customer-level feature matrix\n",
    "# 2. `task-3-features-description.csv`: Complete feature documentation\n",
    "# 3. Visualizations for feature analysis and validation\n",
    "# \n",
    "# ### Next Steps:\n",
    "# 1. **Model Training**: Use engineered features with target variable\n",
    "# 2. **Feature Selection**: Further refine based on model performance\n",
    "# 3. **Monitoring**: Track feature stability over time\n",
    "# 4. **Validation**: Ensure features meet business and regulatory requirements\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 10: FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING PIPELINE COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal customers processed: {feat_df.shape[0]}\")\n",
    "print(f\"Total features created: {feat_df.shape[1] - 1} (excluding CustomerId)\")\n",
    "\n",
    "# Count feature categories\n",
    "aggregate_features = len([c for c in feat_df.columns if 'sum' in c or 'mean' in c or 'max' in c or 'std' in c or 'skew' in c])\n",
    "temporal_features = len(temporal_cols)\n",
    "categorical_encodings = len(cat_encode_cols)\n",
    "woe_features = len(woe_cols)\n",
    "normalized_features = len(num_cols_after)\n",
    "\n",
    "print(f\"\\nFeature Categories:\")\n",
    "print(f\"  • Aggregate features: {aggregate_features}\")\n",
    "print(f\"  • Temporal features: {temporal_features}\")\n",
    "print(f\"  • Categorical encodings: {categorical_encodings}\")\n",
    "print(f\"  • WoE features: {woe_features}\")\n",
    "print(f\"  • Normalized features: {normalized_features}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  • Features: ../data/processed/task-3-features.csv\")\n",
    "print(f\"  • Documentation: ../data/processed/task-3-features-description.csv\")\n",
    "\n",
    "print(f\"\\nThe feature engineering pipeline has successfully created a clean,\")\n",
    "print(f\"customer-level feature table ready for modeling. All transformations\")\n",
    "print(f\"are documented for auditability and regulatory compliance.\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
